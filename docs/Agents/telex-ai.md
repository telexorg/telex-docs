---
sidebar_position: 3
---

# Telex AI

## Introduction

Telex AI is a service layer that provides powerful AI capabilities to agents and applications via simple and standardized APIs. Telex acts as an AI provider, allowing agents to interact with various AI models for use cases such as text generation, streaming conversations, and more.

Much like OpenAI, Telex exposes endpoints for chatting with AI models, retrieving available models, and specifying inference parameters. The API is designed to be flexible, supporting multiple ways of specifying model preferences and handling different interaction modes, from synchronous chat to streamed completions.

## Capabilities Overview

Telex AI supports the following categories of AI functionality:

### 1. Text Generation

Generate natural language text responses from user inputs.

#### Use Case Examples:

* Chatbots
* Code generation
* Summarization
* Q&A systems

## Authentication

All requests to the Telex AI API require an API key.

* **Header:** `X-TELEX-API-KEY`
* **Example:** `tlx-***your-api-key***`

---

## API Endpoints
**Base_Url**: `https://api.telex.im/api/v1`

### 1. Model Discovery

Retrieve a list of available models with full metadata.

**Endpoint:** `GET /telexai/models`

**Headers:**

* `X-TELEX-API-KEY`: Your API Key

**Response:**

```json
{
  "status": "success",
  "status_code": 200,
  "message": "all models listed successfully",
  "data": {
    "data": [
       {
        "id": "sentientagi/dobby-mini-unhinged-plus-llama-3.1-8b",
        "hugging_face_id": "SentientAGI/Dobby-Mini-Unhinged-Plus-Llama-3.1-8B",
        "name": "SentientAGI: Dobby Mini Plus Llama 3.1 8B",
        "created": 1748885619,
        "description": "Dobby-Mini-Leashed-Llama-3.1-8B and Dobby-Mini-Unhinged-Llama-3.1-8B are language models fine-tuned from Llama-3.1-8B-Instruct. Dobby models have a strong conviction towards personal freedom, decentralization, and all things crypto — even when coerced to speak otherwise. \n\nDobby-Mini-Leashed-Llama-3.1-8B and Dobby-Mini-Unhinged-Llama-3.1-8B have their own unique, uhh, personalities. The two versions are being released to be improved using the community’s feedback, which will steer the development of a 70B model.\n\n",
        "context_length": 131072,
        "architecture": {
          "modality": "text->text",
          "input_modalities": [ "text" ],
          "output_modalities": [ "text" ],
          "tokenizer": "Other",
          "instruct_type": null
        },
        "pricing": {
          "prompt": "0.0000002",
          "completion": "0.0000002",
          "request": "0",
          "image": "0",
          "web_search": "0",
          "internal_reasoning": "0"
        },
        "top_provider": {
          "context_length": 131072,
          "max_completion_tokens": null,
          "is_moderated": false
        },
        "per_request_limits": null,
        "supported_parameters": [ "max_tokens", "temperature", "top_p", "stop", ... ]
      },
      {
        "id": "deepseek/deepseek-r1-distill-qwen-7b",
        "hugging_face_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "name": "DeepSeek: R1 Distill Qwen 7B",
        "created": 1748628237,
        "description": "DeepSeek-R1-Distill-Qwen-7B is a 7 billion parameter dense language model distilled from DeepSeek-R1, leveraging reinforcement learning-enhanced reasoning data generated by DeepSeek's larger models. The distillation process transfers advanced reasoning, math, and code capabilities into a smaller, more efficient model architecture based on Qwen2.5-Math-7B. This model demonstrates strong performance across mathematical benchmarks (92.8% pass@1 on MATH-500), coding tasks (Codeforces rating 1189), and general reasoning (49.1% pass@1 on GPQA Diamond), achieving competitive accuracy relative to larger models while maintaining smaller inference costs.",
        "context_length": 131072,
        "architecture": {
          "modality": "text->text",
          "input_modalities": [
            "text"
          ],
          "output_modalities": [
            "text"
          ],
          "tokenizer": "Qwen",
          "instruct_type": "deepseek-r1"
        },
        "pricing": {
          "prompt": "0.0000001",
          "completion": "0.0000002",
          "request": "0",
          "image": "0",
          "web_search": "0",
          "internal_reasoning": "0"
        },
        "top_provider": {
          "context_length": 131072,
          "max_completion_tokens": null,
          "is_moderated": false
        },
        "per_request_limits": null,
        "supported_parameters": [ "max_tokens", "temperature", "top_p", "reasoning", "include_reasoning", "seed" ]
      },
      ...
    ]
  }
}
```

### 2. Chat Interaction

Send messages to a selected model to generate AI responses.

#### Headers

* `X-TELEX-API-KEY`: Your API key
* `X-Model`: The ID of the model to use (or specify in the body)

**Model Selection**

There are three different methods of specifying the model when making a chat request to Telex AI. 
* Using the request Header
* Using the qery parameter
* Using the request body

Examples are shown below using differnt scenarios.

#### A. One-Time Message (Stateless Chat)

For single-turn conversations without any previous context:

**Endpoint:** `POST /telexai/chat`

#### Using Headers to specify the model

* `X-TELEX-API-KEY`: Your API key
* `X-Model`: The ID of the model to use (or specify in the body)


**Request Body:**

```json
{
  "messages": [
    {
      "role": "user",
      "content": "Tell me a joke."
    }
  ],
  "stream": false
}
```
**Sample Response**

```json
{
  "status": "success",
  "status_code": 200,
  "message": "chat completed successfully",
  "data": {
    "Messages": {
      "role": "assistant",
      "content": "Why don't scientists trust atoms?\n\nBecause they make up everything!"
    }
  }
}
```

#### B. Multi-Turn Conversation (Contextual Chat)

To maintain context across multiple messages, send the entire conversation history in the `messages` array:

#### Headers

* `X-TELEX-API-KEY`: Your API key

**Using Query Parameter to specify the model**

`POST /telexai/chat?model=${model_id}`

**Sample Request Body**

```json
{
  "messages": [
    { 
        "role": "user", 
        "content": "Who is Ada Lovelace?" 
    },
    { 
        "role": "assistant", 
        "content": "Ada Lovelace was a mathematician and writer..." 
    },
    { 
        "role": "user", 
        "content": "What is she known for?" 
    }
  ],
  "stream": false
}
```

#### C. System or Developer Instructions

Include special instructions to guide the assistant’s behavior.

#### Headers

* `X-TELEX-API-KEY`: Your API key

**Using request body to specify the model**

```json
{
  "model": "deepseek/deepseek-r1-0528:free",
  "messages": [
    {
      "role": "system",
      "content": "You are an AI writing assistant. Always write in a formal tone."
    },
    {
      "role": "user",
      "content": "Write a summary of Telex AI."
    }
  ],
  "stream": false
}
```
:::note
Roles for setting behavioural instructions can be either `system` or `developer`
:::


#### D. Streaming Chat (Live Output) - Note yet supported

Enable real-time responses with token-by-token delivery.

```json
{
  "organisation_id": "org_abc123",
  "model": "deepseek/deepseek-r1-0528:free",
  "messages": [
    { 
        "role": "user", 
        "content": "Write a poem about the stars." 
    }
  ],
  "stream": true
}
```
**Response:** SSE (Server-Sent Events) stream of chunks.

:::note
Streaming is not yet supported
:::


## Error Handling

Standard error responses are provided for all endpoints.

### Authentication Error

```json
{
  "status": "error",
  "status_code": 401,
  "message": "invalid API key",
  "error": "unauthorized"
}
```

### Invalid Model Error
```json
{
  "status": "error",
  "status_code": 400,
  "message": "Failed to extract model",
  "error": "invalid model selected: google/gemini-2.5-flash-preview-05-2"
}
```
### Chat Message Error

```json
{
  "status": "error",
  "status_code": 422,
  "message": "Validation failed",
  "error": {
    "TelexAIChatCompletionsReq.messages": "messages is a required field"
  }
}
```

### Internal Server Error

```json
{
  "status": "error",
  "status_code": 501,
  "message": "Streaming is not implemented yet",
  "error": "Streaming is not implemented yet"
}
```
```json
{
  "status": "error",
  "status_code": 500,
  "message": "internal server error",
  "error": "internal server error"
}
```


---

## Conclusion

Telex AI empowers developers and agents to harness advanced AI models through a clean and powerful API. Whether you’re building chatbots, assistants, or creative tools, Telex makes it easy to integrate powerful language model functionality with minimal setup.

> For feedback or feature requests, contact the Telex team via official support channels.
